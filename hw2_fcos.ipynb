{"cells":[{"cell_type":"markdown","metadata":{"id":"6VBJpAMzglRo"},"source":["# Anchor-free single-stage object detection with FCOS (v2)\n","\n","In this exercise your goal will be to solve an object detection training and prediction task using the anchor-free single-stage approach.\n","\n","There are 10 points to get in total.\n","\n","## TLDR; overview\n","\n","In this task one should:\n","- build an object detection model using the variant of `FCOS`,\n","- train an object detection model.\n","\n","Hints and comments:\n","\n","- Model architecture and loss are heavily inspired by [FCOS](https://arxiv.org/pdf/1904.01355.pdf) paper,\n","- you can freely subclass and extend the interface of classes in this exercise,\n","- be sure that you understand the concept of anchor-free object detection. There are many tutorials and articles about it (e.g. [this](https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c) one)."]},{"cell_type":"markdown","metadata":{"id":"yidImLmW2eSk"},"source":["### Notebook changelog (compared to the initial version)\n","Changed in v2:\n","- Added definition of $\\sigma$ in the scoring formula.\n","- Added the description how the `target` variable should look like.\n","- Fixed the typo about mismatched `in_channels` and `out_channels` in the classification head description and added the whole info about it in the regression head description.\n","- Added information about 1-element batch.\n","- Fixed typehint in `BackboneWithFPN` (`forward(self, x: MnistCanvas)` -> `forward(self, x: torch.Tensor)`)\n","- Removed info about non-existing exercise (\"...so use the foreground mask from the previous excercise.\" -> \"... so use the foreground mask.\")\n","- Fixed typo: \"use `self.box_coder.decode_single` and `self.box_coder.decode_single`\" -> use \"`self.box_coder.encode_single` and `self.box_coder.decode_single`\"\n","- Removed mentions of non-existing `TargetDecoder.get_predictions` and rotation.\n","- Removed additional TODO placeholder from detection post-processing.\n","- Added the information about using the different `evaluate` parameters."]},{"cell_type":"markdown","metadata":{"id":"5PsyO2OdlLLE"},"source":["### Data description\n","\n","In this task we will paste bounding boxes with digits **from 1 to 5** randomly selected from `MNIST` dataset on a canvas of size `(128, 128)` and **randomly scaled by a factor between 0.5 and 1.0**. We assume that:\n","\n","- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n","- the digits are fully contained in canvas,\n","- boxes are modeled using `MnistBox` class,\n","- canvas is modeled using `MnistCanvas` class.\n","\n","Let us have a look at definition of these classes:"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"L1rAdIiRq2G8","executionInfo":{"status":"ok","timestamp":1671109982327,"user_tz":-60,"elapsed":302,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["from typing import List\n","from typing import Optional\n","from typing import Tuple\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","import torch\n","\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class MnistBox:\n","\n","    def __init__(\n","        self,\n","        x_min: int,\n","        y_min: int,\n","        x_max: int,\n","        y_max: int,\n","        class_nb: Optional[int] = None,\n","        rotated: Optional[bool] = None,\n","    ):\n","        self.x_min = x_min\n","        self.x_max = x_max\n","        self.y_min = y_min\n","        self.y_max = y_max\n","        self.class_nb = class_nb\n","        self.rotated = rotated\n","    \n","    @property\n","    def x_diff(self):\n","        return self.x_max - self.x_min\n","    \n","    @property\n","    def y_diff(self):\n","        return self.y_max - self.y_min\n","\n","    def __repr__(self):\n","        return f'Mnist Box: x_min = {self.x_min},' +\\\n","               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n","               f' y_max = {self.y_max}. Class = {self.class_nb}.' +\\\n","               f' Rotated = {self.rotated}.'\n","\n","    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n","        ax.add_patch(\n","            patches.Rectangle(\n","                (self.y_min, self.x_min),\n","                 self.y_diff,\n","                 self.x_diff,\n","                 linewidth=1,\n","                 edgecolor=color,\n","                 facecolor='none',\n","            )\n","        )\n","        ax.text(\n","            self.y_min,\n","            self.x_min,\n","            f'{self.class_nb}' if not self.rotated else f'{self.class_nb}*',\n","            bbox={\"facecolor\": color, \"alpha\": 0.4},\n","            clip_box=ax.clipbox,\n","            clip_on=True,\n","        )\n","\n","    @property\n","    def area(self):\n","        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n","\n","    def iou_with(self, other_box: \"MnistBox\"):\n","        aux_box = MnistBox(\n","            x_min=max(self.x_min, other_box.x_min),\n","            x_max=min(self.x_max, other_box.x_max),\n","            y_min=max(self.y_min, other_box.y_min),\n","            y_max=min(self.y_max, other_box.y_max),\n","        ) \n","        return aux_box.area / (self.area + other_box.area - aux_box.area)\n","\n","        \n","    def get_torch_data(self):\n","        return {\n","            'box': torch.tensor([self.y_min, self.x_min, self.y_max, self.x_max]).to(DEVICE),\n","            'class': torch.tensor(self.class_nb, dtype=int).to(DEVICE),\n","        }\n","\n","\n","class MnistCanvas:\n","\n","    def __init__(\n","        self,\n","        image: np.ndarray,\n","        boxes: List[MnistBox],\n","    ):\n","        self.image = image\n","        self.boxes = boxes\n","        self.shape = (1, 1, self.image.shape[0], self.image.shape[1])\n","\n","    def add_digit(\n","        self,\n","        digit: np.ndarray,\n","        class_nb: int,\n","        x_min: int,\n","        y_min: int,\n","        rotated=None,\n","        iou_threshold=0.1,\n","    ) -> bool:\n","        \"\"\"\n","        Add a digit to an image if it does not overlap with existing boxes\n","        above iou_threshold.\n","        \"\"\"\n","        image_x, image_y = digit.shape\n","        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n","            raise ValueError('Wrong initial corner box')\n","        new_box_x_min = x_min\n","        new_box_y_min = y_min\n","        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n","        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n","        new_box = MnistBox(\n","            x_min=new_box_x_min,\n","            x_max=new_box_x_max,\n","            y_min=new_box_y_min,\n","            y_max=new_box_y_max,\n","            class_nb=class_nb,\n","            rotated=rotated,\n","        )\n","        old_background = self.image[\n","            new_box_x_min:new_box_x_max,\n","            new_box_y_min:new_box_y_max\n","        ]\n","        for box in self.boxes:\n","            if new_box.iou_with(box) > iou_threshold:\n","                return False\n","        self.image[\n","            new_box_x_min:new_box_x_max,\n","            new_box_y_min:new_box_y_max\n","        ] = np.maximum(old_background, digit)\n","        self.boxes.append(\n","            new_box\n","        ) \n","        return True\n","        \n","    def get_torch_tensor(self) -> torch.Tensor:\n","        np_image = self.image.astype('float32')\n","        np_image = np_image.reshape(\n","            (1, 1, self.image.shape[0], self.image.shape[1])\n","        )\n","        return torch.from_numpy(np_image).to(DEVICE)\n","\n","    @classmethod\n","    def get_empty_of_size(cls, size: Tuple[int, int]):\n","        return cls(\n","            image=np.zeros(size),\n","            boxes=[],\n","        )\n","    \n","    def get_boxes_and_classes(self):\n","        boxes = []\n","        classes = []\n","        for box in self.boxes:\n","            box_data = box.get_torch_data()\n","            boxes.append(box_data['box'])\n","            classes.append(box_data['class'])\n","        boxes = torch.stack(boxes).to(DEVICE)\n","        classes = torch.stack(classes).to(DEVICE)\n","\n","        return {\n","            'boxes': boxes,\n","            'labels': classes\n","        }\n","\n","    def plot(self, boxes: Optional[List[MnistBox]] = None):\n","        fig, ax = plt.subplots()\n","        ax.imshow(self.image)\n","        boxes = boxes or self.boxes\n","        for box in boxes:\n","            box.plot_on_ax(ax)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NWMxgsgFtlze"},"source":["Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"HezSZXw4z-cx","executionInfo":{"status":"ok","timestamp":1671109983870,"user_tz":-60,"elapsed":1277,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["from keras.datasets import mnist\n","import numpy as np\n","import skimage.transform as st\n","\n","\n","mnist_data = mnist.load_data()\n","(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n","\n","\n","def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n","    bool_digit = digit > threshold\n","    x_range = bool_digit.max(axis=0)\n","    y_range = bool_digit.max(axis=1)\n","    start_x = (x_range.cumsum() == 0).sum()\n","    end_x = (x_range[::-1].cumsum() == 0).sum()\n","    start_y = (y_range.cumsum() == 0).sum()\n","    end_y = (y_range[::-1].cumsum() == 0).sum()\n","    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n","\n","\n","TRAIN_DIGITS = [\n","    crop_insignificant_values(digit) / 255.0\n","    for digit_index, digit in enumerate(mnist_x_train[:10000])\n","]\n","TRAIN_CLASSES = mnist_y_train[:10000]\n","\n","TEST_DIGITS = [\n","    crop_insignificant_values(digit) / 255.0\n","    for digit_index, digit in enumerate(mnist_x_test[:1000])\n","]\n","TEST_CLASSES = mnist_y_test[:1000]\n","\n","\n","def get_random_canvas(\n","    digits: Optional[List[np.ndarray]] = None,\n","    classes: Optional[List[int]] = None,\n","    nb_of_digits: Optional[int] = None,\n","    labels = [0, 1, 2, 3, 4]\n","    ):\n","    digits = digits if digits is not None else TRAIN_DIGITS\n","    classes = classes if classes is not None else TRAIN_CLASSES\n","    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n","    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n","    attempts_done = 0\n","    while attempts_done < nb_of_digits:\n","        current_digit_index = np.random.randint(len(digits))\n","        current_digit_class = classes[current_digit_index]\n","        if current_digit_class not in labels:\n","            continue\n","        rescale = np.random.random() > 0.5\n","        current_digit = digits[current_digit_index]\n","        if rescale:\n","            factor = (np.random.random() / 2) + 0.5\n","            current_digit = st.resize(\n","                current_digit, \n","                (int(current_digit.shape[0] * factor), int(current_digit.shape[1] * factor)))\n","            # current_digit = np.rot90(current_digit)\n","        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n","        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n","        if new_canvas.add_digit(\n","            digit=current_digit,\n","            x_min=random_x_min,\n","            y_min=random_y_min,\n","            class_nb=current_digit_class,\n","            rotated=rescale,\n","        ):\n","            attempts_done += 1\n","    return new_canvas"]},{"cell_type":"markdown","metadata":{"id":"5i2OjUEC7eaC"},"source":["Let us have a look at example canvas (rescaled digits have additional *added to description)."]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"OsLpINOtvhd8","outputId":"76010ff3-6446-43ba-fe88-43275d46ccfb","executionInfo":{"status":"ok","timestamp":1671109983872,"user_tz":-60,"elapsed":14,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mnist Box: x_min = 13, x_max = 24, y_min = 2, y_max = 8. Class = 1. Rotated = True.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZQc5Xnv8e9TVb1O93TPrtHMSDPSaEUYSwgQiABmM8a+BifYxsYOtkmU5GZ1YjuQ3HMd557cE9/4OLaTGztcb9x7MNiWsSHYAbPIxgYjEAIkoX3XjGbf116q3vtHN9JIGiEx3TPTo34+5+hMd3VN9TOl6d+89dZbb4kxBqVU8bJmuwCl1OzSEFCqyGkIKFXkNASUKnIaAkoVOQ0BpYrctIWAiNwiIntEZL+I3Dtd76OUyo1MxzgBEbGBvcBNQAvwMvARY8zOvL+ZUionzjRt93JgvzHmIICIPAzcBkwaAn4JmCAl01SKUgpgiL5uY0zV6cunKwTqgGMTnrcAV0xcQUQ2ABsAgoS5Qm6YplKUmnskEMAKBRm9cinpEovSX+zHGxrGJBJT3ubTZuORyZbPWsegMeZ+Y8xaY8xaH4HZKkOpgmSVlmIa6/B9pp3b/u5p0kvqscvi0/Je09USaAUaJjyvzy5T6sIjgr10ManqKD2rgpTvTuC88AYmmYSp9rklEliDowyMB+lLleAFbQj481t31nS1BF4GlohIk4j4gTuBx6bpvZSaXWIx3hCjd0WQ4WtH6FkRxIqUILY95U2adBpGxxgaDdKVjGIsAWt6Pq7TslVjTBr4E+BJYBfwA2PMG9PxXkrNNsvvo/3KAMM3jPDQFd9k/F1DDFy/BLuqcsrb9MbGcLt7cF6Ksum5dxDc2Yp7vD2PVZ80XYcDGGN+BvxsuravVCFxg4ZIeJxVfqG+vJ+u2ijxYA59XcZgXBdjgfEZvIo4VtrF7erKX9FZOmJQqRwZY3CGheHRIK4xXBRvY2BlGi+W+2nv0ZXjXLPuDVpuLWfkiqY8VHsmDQGl8kA8cN3Mx8nGy+OGoTowhHVVH90XT0/DXUNAqTyw0uClp+HjZKDcGeF/rvoxYyvG8799NASUyp3rUrEjSXBvEC+frQDAaQvwn20XMe75ETGI44BIft8jr1ubojDjpMyOvG0vQZCINOdte0q9FeMZAp1jBPr8DHlpUmbqpwZPF+wSjraXYzd5BMNJrMYG6OzBHRzM23sURAhE8biNcN629yijeduWUudkPOzOPsKdEX41XsfxsVietmuof6KHgWNllF89zCeWvci/fe56mn5Qju/pV/LzHujhgFJ54VXGGC+zqHP6uLj0OPMW9uCGfblvuLuPcEeCneN1+MSlbkEP4xU+JJC/ofYaAkrlSGybkcYoww1wiT/JR+Iv8+nFT5OM5z7M1+3oxH+sj1/3LyHh+fjt+tcYq7SwohGw8nPYURCHA0rNZcYzBHqS+Aczh7RVlnCRvx3Pd2YHnvj8iM/JdPBZ2derK/FiYexjnZjxU88AeGPjkErzwr5FpJst/r7+P/j6+ms4bi9j/vf24Hb35Fx/QYfAwxxhJwNEcPgcK08sf4kemolQhh8hvz2lSk2FPZbCGQMXQ9jyUUUSYwuIII4PsS3w+bDK45hwEBPwYbK9/GP1JYyV28QCNvZI6tTt9g/jRUMw5GM4FWCxE+KShha29i2mLhjMS+0FHQKXUc7VVPE9DgPQT5InaaMMPwcZ5iAjfIgFs1ukUsbDGk9jpU5eMWgBrl+wK8pJLW9gvDrAUL1N8ppBbmzcw7WleyixMnMDBCWFT9KMeAG8CUfoLsLXjtxI5zB8oP5lLoscyqxvpzE+D+z8HM0XdAgsJkovJydRiOPnVubzFfZQS5BPsXgWq1NqchYWNkLPKsH1LWVwkZCMe1A1xh2Ld/D+2FZW+9MExMHDMOwlGDcelbbBQkjj0uUmaEmHeHfNTgYqQ7wjdIxGXzfgYIlHPhvABR0CpxsgyRO0cTkVVODnEY5xh7YEVIGxECJWgC988GEGvRAfjR4kJKd3EmYCIGFS7Ev7OJaq4PpQOxErwKiX4pdjC/lRxxrubfgZl/rtE98zHeZUCMTw82EW8hI9LCLCpZTPdklKZSYO6e4j3Bnn+0OLWRc6yEU+P2uCLYxnBw49NRbi2+1X0zUWYWAsyMD+MuykgAfOmGCl4G+iBmODpME/KAR6DR9e/4c01nfzsfrNLA8cZ10AfrtyK76LPY6XLUDaAzlNOQZzLAQyPaoWV8h8cN3MxAtKFQCvf4Bgd5Ln+pZSYQ9zka+fhY6flHHpctM8MXAxW15ZQqDHItAPy37WBgPDYDxIpzGuhwT8IBZ4LiaZwown8I2soX1JHT++fjXXV4ZZF9jPzaFe5tds4r7yPyQQKcEtjhAQrNIo3ddUk65KEQylMAfCxJ48MNuFKQWA+P0YW+hNhBnygngY/qnnYjZ1LqXz5/WUHnJZ8Uo7pNKQTuP29mFSE/6IGQ9GrFOfG0PZU/so2xKj80gj/3JlA+97z3aqLGGe7XHsRj+V1UuJfn/z1Kcxo0BD4FPsIEaSjwC/ALqBfxr8JV94HO45bd0B/HybVTNeo1Kn8DyckRS7DtfyoHcFXTU7eXDPZaSOlrBga4LgsQHSBw+/9TaMe8Yit7sHGRqmrCLCWE2EB/sv5wOlW5nvGFLVKcYq/URzLL0gQyBGkn9mDZeTuYGBXVFGuiJC9IMtJGOHsLD4/L6rqPw/LXyarbNdrlJ4IyPIa3tY+d8q8SpjPFF5HYveaMHt6cOkU5z58T5/JpGAzdtpGFrKYyPXcuhjFXy1/gmqagcYaKvMHEJMEiDna04MGzZ+H6mIQ9hO4mBjIfm+mlKpnJl0Cq+vH2nvIXioB69/AJPKzjic652+jEHGEgT7PAaTISwRbpy/h3TTOE5dLVZ06u2BgmwJnM4L+khGhZCVOvfKSs0WY/BGR2F0mq5iTaYI9LsMJDMjBe8pf4HDTRV0NyzA53l4Q0NT2mzhtwREGF4QIHppL0sD0zPbqlJzgdvVTfjlw3Q8V8d7d9zFuLFZVNLNSH0IE596S6DgQ0AsIRUR1sUPEZP83zxVqbnCJBK4XV1EjhpaD1aSMhblzgiJmOCFpn7ZckEcDgxhnTIRSAnZiUEsC/H56Un2Ut3Vw6tiIWQOCR7v6ifO6Ml1J0gQJA9XcitVkMp2DYNEaL05RqUzyOAiiB4LMdULlwsiBEYJ4pOTp/k2mN38UFZhRaKY5Y0M3OXnD24+hC0nGy5feu5Gqn6y7cS6E2kAqAuZ0zVIpMXPkBsibo+SmpckWeqbcggU9OGAFSula3WEqurBUwJAqWKWPnSE4PZj9LgR5jkDrF58lLGKqZ8um/InS0QaRGSTiOwUkTdE5M+zy8tF5CkR2Zf9WjbV93Br4ozeNMz7G7afWNaWHuaeo1cj+3K/sYNSc1Y6zfeOXsZPB9/JLVVvkIjPQggAaeCvjDErgXXAH4vISuBe4BljzBLgmezzt018flLxIB9ofp1rIrtPLO/1bDbtXUqkJYfKlZrrUmmOH67klx1Lcr7ZyZT7BIwxbUBb9vGQiOwC6oDbgOuyqz1AZuTvX7/d7XtrV9BzUYDLIwdpsIeBCK7xeC1RT/M3XHxHjqCXD6li5Q4NsfJ/tEDAz4+C17Cwa9+URyXmpWNQRBqB1cBmoCYbEADtQM1ZvmcDsAEgOGG6cfH5IQkdl5cwuCpFg9NL1DrZ1HGNYKU8vYJQFTdjSLcez8umcg4BEYkAPwL+whgzKBPG8xpjjMjkJ/eNMfcD9wOUSvmJdax4DDrhkxt+xobYXsKWHyb0e8btUfqXlRA31dDRmWv5ShW9nEJARHxkAuBBY8wj2cUdIlJrjGkTkVrg7X1Sy0qhM/NhD0imvLb0MMfcAP/90O3sOVTL0r0j2O19ejigVB7kcnZAgG8Bu4wxX57w0mPA3dnHdwOPvp3teqUhAKLW+InTgl2ew9axRg49v4DanzvYe4/iduT/Pu15I5I5rMnTvPBKTadczg6sBz4OXC8ir2X/3Qr8I3CTiOwDbsw+P29DTZlTf/Oc/hPLUsZiyAtS9bpH/BcHcQeHM1dnFSDx+bEuWsbBv7+U8Vsvne1ylDqnXM4O/Jqzz3l6w1S3aycz3QNfb7+eTZHMBUO7h+ext6+K+PFx3M6u3C/LnC4i2BVljCyIUn5JF8OHqsjPzPBKTZ+CGDY8UfinmUlCet7t8oJUA2CMS4V3PHM3lkINAEAcHwPrG+m8zOKXq77LNds/S+VsF6XUORRcCJh0mnbCPDn4nfNavz2PdzPOldgWA4ttvAWjRC0HoyOd1RxQcCEA8HG5dbZLmBqfj+FlSdY3HsYq7MsylDqhIENgLrLjMaipYv2K/Xyy5lfYOv+ZmiP0z1WeSCRCujLC2thhLvEPznY5Sp03bQnkyfiyeXRfHGB5oI2YlT0noDMhqTlAWwJ5MrjQz+BFKarsqU32qNRs0ZZAnvRcYvj8bz3KQieF9+bwCaP9AqrwaUsgT0w0zVWhQwRFhwqruUVbAnniBNM0OZm+AC/HSR6UmknaElCqyGlLII8sBA9zcqCQnh1Qc4C2BPJExGCLhXXWa6qUKkwaAnlijOAaDw/966/mFj0cyKM3A8DDwzVGTxGqOUFDYBocSSf5bu+VhDo0BFTh08OBaTDqOeweqsEZm+1KlDo3DYFpMGJ8tI2UYie0f0AVPg2BPJp4ZsBof4CaIzQE8mTiKUJbzxCoOUQ7BnPkzKthfGU988p7T5widHWsgJpDtCWQq3CI4To/8eCYjhFQc5KGgFJFTkMgR8a2cAPgt/SmaGpu0hDIkQn4SZQLMd/4bJei1JTkHAIiYovIqyLyePZ5k4hsFpH9IvJ9EfGfaxtzmmPh+iFkp/BlJxTp98J0HI8TGNQ+AlX48tES+HNg14TnXwT+2RjTDPQB9+ThPQqWEcHzGxzLBTJjBUa8APaAgzOmk4uowpdTCIhIPfBe4JvZ5wJcD2zMrvIAcHsu7zEnGMGbcBXhpYFWPn7jc/Su0DOwqvDl2hL4CvA5ODGfVgXQb4x5s5esBaib7BtFZIOIbBGRLSkSOZYxe8R1scch4TknThHGLOFd0Z2kIno4oArflENARN4HdBpjXpnK9xtj7jfGrDXGrPURmGoZs84aHifSamgbi51YFrX8XBFIkQ7NYmFKnadc2qvrgfeLyK1AECgFvgrERcTJtgbqgdbcyyxcZmCQsp0RjvxkEcsW/1cmjheq+432CajCN+UQMMbcB9wHICLXAZ8xxtwlIj8E7gAeBu4GHs1DnQXL7emFnl7mbYF5s12MUlMwHeME/hr4SxHZT6aP4FvT8B5KqTzJS/e1MeYXwC+yjw8Cl+dju0qp6acjBpUqchoCShU5DQGlipyGgFJFTkNAqSKnIaBUkdMrXPJFBPH7sRYtIB0PM1oXRFywkx7hzQdxu3tmu0KlJqUhkCdWKIQVK+XQ71SRvmiEjVd+lePpGDvH6/jpn70L51kNAVWY9HAgT6zqSpKL55FaOcr6poPMt11W+nu4KrwPz6+7WRUubQnkiRePMFob4LKFe3lP+TZiVpAyhJiVwHN0CnJVuPRPVJ7IaAL/kMeRoTIOJapJGZc2d5TXkyGstM4roAqXhkC+eB5WyiOZdhj1MtMqdrs+difmY6X0kmJVuDQEpoGVnVTgpfEmvt+6FntUpyNXhUtDIB8sG7cyyvB8P0vKumgOdjBsUjzbu5zD2+fjDOg9ylXh0o7BXIkgPoexqiDD9cL6sv1cHGil34Otx+qp2QzSOzDbVU6NnNahabRv40KkIZAjOxqFhlpar7VYd9VOrgvvZcQ4fLX9RuydEeKvduINDs12mWdll5UhJWFM0A+WBY5NYl6UsSof/c0W6ZLMB98/KAS7DNW/bMfdf2iWq1b5pCGQI4lGGG0oxd84zO9WP0/McjmQqODF1oWUtBno6MIbL7zZlMXnx4pF8RbOI1EZwg1ZeLZgbGG41mKsxhBa2UdtZASAtv5SettLKN9VinXYwaS1n+NCoSGQC8sm2VzD4duFTyzZyhXBQb4/tJTvHbucsociRHf34Q4Og+fOdqVnkIuaOXpLnMrrj/P7C5+gwdeDn0ydQUkTEJe45WFnDwmSxjBqhA8e+Cx1nQ24B48W5M+l3j4NgSkSx8FqWkDv4iAXrzjEilArQ57L0z0rOHK0kqUtY1j9Q3gF9EERnx+rJERqVRNdq0Kk1wzx3vnbuSp4hC4vQJcbZcvIIhKeQ8JzsMRQ5oxya3QblXaKhY6f4YUeQ++oJnK8A290dLZ/JJUHGgJTJKEQfWur6b7MZdPin9DrpTmSDvPyjsXE3nCw9xzALbAPiRUpgfnVHLgzwOIVLfyo+SHKbRsbh41Dy/ll91J2v9yIMwb2WLYFUGYYuCnEu6K7uCk0xrJLjrLbbmD5cyEosJ9PTY2GwBTYFeWY+hp63j/Ge5r34BObrYlKnh1YQXybQ/meBN7oKCZVWMfN/Tcvo2+FxZ3rf826yH7KbZsfDy/k2b7lvPbDVUSOezS1jiNpDyuVacGMV4X4YckVHLqkgpuanqIx0sveimqw7Vn+aVS+aAi8FZFJT4tJrJTR+RHuWvk8745uB2DnWB0vdy0gdihF4GgfbjJZWKfURBhssjArh/h42YvELY9eF37ecxEv7lvE8sfa8A4fO9Hh92bl4ZpqSpub2V1XDU1QH+ijrHQU8fnOun/U3KIhcBZ2WRlSGsE93oFJJTMLRRDbpvO6WnouMXwgtpUqK82upI8Hdl1BYHOE8p3H8Dq6Cu/DYQwlxw39kQh3BT9Jf2+EkjcClO1Ns+zIMKalbfIef89gJwzJdOYv/4djrxBtGuexxnfhT6dJt3fM8A+i8k1D4GxcF9KndupZ4TBWWZyhRog19lNlpXGBw+kKUh0hqo64mMFhvEThnRIEKDmexLP9DLkVRPugfFeS0KE+6OzGPVvN6TShHo++kcz1EJW2zaJAB6O1AZz+MtAQmPM0BM7CHRyEoaFT/qJL3TwG3lFJ3VWtfLbxSWKWn10p+MXgcmJ7bGIvHMQ97XsKie/pVygXoVwsMJmLmtxz1OoODFL63EG6VjcDEJEAjU4fXWssjBUnumPay1bTLKdrB0QkLiIbRWS3iOwSkStFpFxEnhKRfdmvZfkqdsZlPyDiODh18xl8RyXHr4F3Ve9lka+XXi/Jw31X8MRPL6Ni5zje4BDGLZxTgpMyJnN+35jzCyvjYYaGsFKnDSGW7D815+V6AdFXgSeMMcuBS4BdwL3AM8aYJcAz2edzmvj9uPPK6Ftic9XaPdwc3c582+a4G+DZ1iU0/agf//YjeCMjBdsKmDJj8MbHkcI60aHyaMohICIx4BqyNxw1xiSNMf3AbcAD2dUeAG7PtcjZJD4/VnkZ3WtKGV2a4H0VrzPfTtDhpvnY5ntI/rIS9h/F65+jFwmpopdLS6AJ6AK+IyKvisg3RaQEqDHGtGXXaQdqJvtmEdkgIltEZEuKwuxIA5BgAFNawsh8obxyiEX+TkrEImks0p0hQt0Gb2Tkwh1LL4IEApjThwVcYA2eYpZLCDjAGuDrxpjVwAinNf2NMYaz/LoYY+43xqw1xqz1EcihjOklNZWMNsWouqqNu5q2sNpvUWoF8RDEE+QCnzRI/H6sxQtJxU79QcXjgv/Zi0UuIdACtBhjNmefbyQTCh0iUguQ/dqZW4mzyLJxyyOM1DhcUXWYlcFWLISESTPk+fH3WfiHLuxPghWN0H1ZBdb8zMQoB9Jj/Gq0mdheiBzVYcMXgimHgDGmHTgmIsuyi24AdgKPAXdnl90NPJpThbMlOzBorDbEyHzhg2UvsSbQi4chYdL0eCWE2w3BrsI9lMmLeCldV6f5raYDALw8voAnulZR9as25JXds1ycyodcxwn8KfCgiPiBg8AnyQTLD0TkHuAI8KEc32NWWIEAEo3Sep3FglWtNDpJIuID4IVEOY90r6Xy1WHsIx0U+EnBqRHBvW41HauCfGb9f7A+tB9bAvzLgXfR+1oVzYN7MenUbFep8iCnEDDGvAasneSlG3LZbkGwLMSx8SIuMf84L45X4RcXF+GR7rX8pqWRptYe0l0X3p2FxHGQUIjuJQEGl7l8OLqboNi0pIfpOFpO1V4wY+MX3unQIqUjBs/CGx3FG0+w4m9cUgE//+676eQFM2mXplQX6c7uC3JiDWtJE8PLyqi68yifrXuBqOXngcGF/Ouea1n4mCH0qx06l8AFREPgrXgublfXbFcxY8RxsCIljDSX0X2xw0crD7Dc386elOGxjktwXyojdLQXb6hw50xUb5+GgDpBQiFMYx3t62yuvmEbH4q9QlgM3+i9kr2/aWTxl7cW7MVRauo0BBSQGRnJwjoO3hGj9rI2Plr1G8aNza9GG9n42NXM2+riJVPaD3AB0puPFDsRsGys0gjj9VFqL2/jow0vcXVwnF43zK8HllC/KUH09Y631/+R3a4qfNoSKHLO/FrceWXs+1Ap4WX9PLz0ewDsStp86tefoOSNIA2v7cIdHjmv7YnjYFWUIyVhvHAQDh3LXFilCpaGQDESwa4ox2uYR39zhJFam4qLO7m2dj/NvgDPj/vYNLyC0J4gZXvTeENDGM8ggQBi22fcmUj8fvD7kJIwbqyEwcURkhGLdAgqY0F8bf2kDx3RQ4kCpSFQbESwwmFGrlxM192j/N7yp/hQ6TbiloNPbMDia603sOPXzTRv7MDddxBjDFY0ilVRhgkHMb5Tm/mJyjCJMofe5TZjjUm+dM1DNPq6qbGT3PjiH2Ftm8/CL3fqacUCpSFQBKxoFKs8TrKhgkSFn94VDiPNSe5ZuoWrS/ZQbvmzAZBxTcU+OtZEOeDV4O/PXASajkAy7uGFXHBO/Ytuh9IEAuM0lvextLSTiwNt+DAkDCTbw1S2mMKfbKWIaQgUASseI9lYScelIUYaPO647gXWR/bynvAQk/UN3xbZwZqlh/lN3RJ60yUALA22c1noMA22R8Sa/KpPjzcvpvLT4SY4kiql5KhNbP9oZs5GVZA0BIpAx7sbWPSJvdwcP8xCfzfvDBwnbgEEJ12/xvYTt0ZpKH31xMc6LBC1HPo9j970GO5pc4uNG5t/6biB1tEYHcMR+veVU7ZLaHi+G4534F6o8y1cADQEikCiTLh73vM0On1ELZcRz6LLFbrc5Bnrugj9XpB+N37Ga7Z4vD66kJ5UCe5pLYik5/DMnmUw6MM3YFG101C+tRtzpFX7AgqchkARCPQZHmhfT9BOkfZs3uiah2fOnCXU84Rkwod1METpwUk2ZKDitUGs412Z5v2bvf2Og1gWy9PHIJ3GJJOYdBo3lb4gr6240GgIFIH4gQTbn16GsQwYwT8A1iRzoVgG/GmItLmEj49Nui3rWDtuT++pp/uycy8AGM/oB3+O0RAoAvamrSzYlJ9tTfrxNubCnWOxCOiwYaWKnIaAUkVOQ0CpIqchoFSR0xBQqsjp2QGlzkICAWRJE2MLovQ3+xhe6OHGJjkL4gplrzqUHksT+NmWOXe1pIaAUpOxbKxICUNLY/RcZFN6ZSf/sPRxbg6dOTfCsJdgfcUGxl8vZcHPfZjUmSMxC5mGgFKnE8FceTGdK0Ks/v1tXFZ6iHWhgzTYHkxyy7yAONy76gm+XnItVmM9dPXgzqEb1GoIKDWBFQwikRI6V4ToW2n4eNXzLHKGqbFDwMQrJU+yRXhnsIVLK1t4dfVqSnf7YQ6FgHYMKjWBLKxnZN1iGu/ex3du/wZr/ckTAfBWlvlsPl39DFfdu5mj7y2fgUrzJ6cQEJFPi8gbIrJDRB4SkaCINInIZhHZLyLfz96iTKk5wQsHGC+zaSrpodEZxpYzL7Q6m5gl3FS6g1Tp3OoYnHIIiEgd8GfAWmPMKsAG7gS+CPyzMaYZ6APuyUehSs0EL+wjERcWBHqptDJ/v948BJjsUGCiqOXn2tAo6cjculN1rn0CDhASkRQQBtqA64GPZl9/APg74Os5vo9SM8LZf5za4Qq+vOpmfrL4EpbFOnHExRJz4vJrn7isDB/nY6XHZrna/JhyCBhjWkXkS8BRYAz4OfAK0G+MefNkagtQN9n3i8gGYANAkPBUy1Aqr7zefqxEguieizg8Np8j1eVYctqcio5H6/y4hoCIlAG3AU1AP/BD4Jbz/X5jzP3A/QClUj63DqLUBcukkrgDKer+/XXE54B92g1URJDSKDtvXw5/9eTsFJlnuRwO3AgcMsZ0AYjII8B6IC4iTrY1UA+05l6mUjPImLPeMEV8fmisJX0BNV5zOTtwFFgnImEREeAGYCewCbgju87dwKO5lahU4ZBggO7VUcYWza1RgW9lyiFgjNkMbAS2Atuz27of+GvgL0VkP1ABfCsPdSpVEMTvY2AJNNT1zHYpeZPT2QFjzOeBz5+2+CBweS7bVWpWiIBYiHXq2ADjGTDZ036Og1uTYHGs+4xvd41hwEsi6fMfW1AIdNiwUmSuGLSrKjHRMF7JqdcH2GMpZGQMMzyClIQJliQp953ZZ7AtafMH236f+C4NAaXmDhGcxgW4FVH6miOMx4VU6akfYmcU/AOGYL9LOig0Vx1hSajjjE0NekEGe0qYNzy3TnZpCKiiJn4/bbfMZ2Cp4fZrXuL62E6uDfafss6OpI9nh1fyYl8TibTDvzZtpMY+82rCIS+E0+3DNzK3plzXEFBFxyopQWqrab9xHv0rDAtXHmddvINb46+zyBkgIKdeMLTIN44vup3mYDsp4xCzbKwJfeoeHkfSSTYNrKDuuTThfb2TT81eoDQEVNGxSqOMLq7AvKeP/7jkOzQ59oS7Mp95xWCFFaI6YPFOfx8eBjjzmrh9qQpe7akj+vQ23ERien+APNMQUMVDBCsSoe+aRpr/YiefqXqJeodzXinokTk7kAmAMw15Sf70hY8SeT1I1D06HZVPKw0BVTTE8SHzaxhqsPiTec/QYCcISubY/q2uELSwTgTA6esNeUla0g7+Q0FKj7iZ04lzjIaAKg6WjVUe58hvV2MuHWSVz2DLmZ17b9cL41X8vP9ial5OE9neRnoO3odRQ0AVBae2hiF74qYAAAjTSURBVFRjNfYVfbx7we63NVnIW+l1I7SMxvH3JzEDg3nZ5kzTEFBFId1QSe/yEN+55H5W+fM3mKcrHeX4cCmVfWNzanLRiXSOQaVycFPJTv5g0a84+l8qGH/f5Zmhx3OMhoBSOVjouFwVOsjI0iT9zQ5y+vwDc4CGgFI5CFs+GhyLL1z1E+zre86chGQO0D4Bpc6h2x3jmBvgtfGFDLghfiu8lyp7jHrn5NmFA4kahoZDVOspQqUuPHtSpWzsvYyfbr8Yu8/h0DVVrC/dxx2RdgDGjcsjBy/B2RM+ecnxHKIhoNRZDHlJnh2bz8bOtby8YzHVz9tEWxK8tHM1L5Ss4R9KM+uJB2V7XULtoxhXxwkodUFIGZcuV/jP3nfwyuEFxHc4VLzUibtnP1Wbw2DbiN+XWdkzeKOjmFR6zt2RGDQElDqDawzfGVjGI8ffiflKNUtbR5DWFrzsOABvbCyzokzoV5+DIwXfpCGgioI9lCDcHeTeg79DbfitB/V4xuLlYwtwj5awbFcbprsXd3DCaMA3/9qbufvBn0hDQBUF9409hHYK8lObTjn3mfEmsxPjmTl5LcDbpSGgiocxmHT63OsVGR0spFSR0xBQqshpCChV5DQElCpy5wwBEfm2iHSKyI4Jy8pF5CkR2Zf9WpZdLiLyNRHZLyLbRGTNdBavlMrd+bQEvsuZtxy/F3jGGLMEeCb7HOA9wJLsvw3A1/NTplJqupwzBIwxzwG9py2+DXgg+/gB4PYJy/+vyXiRzG3Ka/NVrFIq/6baJ1BjjGnLPm4HarKP64BjE9ZryS47g4hsEJEtIrIlxdyap12pC0nOHYPGGANnmZD9rb/vfmPMWmPMWh+5z/qqlJqaqYZAx5vN/OzXzuzyVqBhwnr12WVKqQI11RB4DLg7+/hu4NEJy383e5ZgHTAw4bBBKVWAznntgIg8BFwHVIpIC/B54B+BH4jIPcAR4EPZ1X8G3ArsB0aBT05DzUqpPDpnCBhjPnKWl26YZF0D/HGuRSmlZo6OGFSqyGkIKFXkNASUKnIaAkoVOQ0BpYqchoBSRU5DQKkipyGgVJHTEFCqyGkIKFXkNASUKnIaAkoVOQ0BpYqchoBSRU5DQKkipyGgVJHTEFCqyGkIKFXkNASUKnIaAkoVOQ0BpYqchoBSRU5DQKkipyGgVJHTEFCqyJ0zBETk2yLSKSI7Jiz7JxHZLSLbROTHIhKf8Np9IrJfRPaIyLunq3ClVH6cT0vgu8Atpy17ClhljHkHsBe4D0BEVgJ3Ahdlv+ffRMTOW7VKqbw7ZwgYY54Dek9b9nNjTDr79EUytyAHuA142BiTMMYcInNj0svzWK9SKs/y0SfwKeA/s4/rgGMTXmvJLjuDiGwQkS0isiVFIg9lKKWmIqcQEJG/BdLAg2/3e40x9xtj1hpj1voI5FKGUioH57w1+dmIyCeA9wE3ZG9JDtAKNExYrT67TClVoKbUEhCRW4DPAe83xoxOeOkx4E4RCYhIE7AEeCn3MpVS0+WcLQEReQi4DqgUkRbg82TOBgSAp0QE4EVjzB8aY94QkR8AO8kcJvyxMcadruKVUrmTky352VMq5eYKuWG2y1Dqgva02fiKMWbt6ct1xKBSRU5DQKkipyGgVJHTEFCqyGkIKFXkNASUKnIaAkoVuYIYJyAiXcAI0D3btQCVaB0TaR2nmst1LDTGVJ2+sCBCAEBEtkw2kEHr0Dq0jumtQw8HlCpyGgJKFblCCoH7Z7uALK3jVFrHqS64OgqmT0ApNTsKqSWglJoFGgJKFbmCCAERuSV7n4L9InLvDL1ng4hsEpGdIvKGiPx5dnm5iDwlIvuyX8tmqB5bRF4Vkcezz5tEZHN2n3xfRPwzUENcRDZm7ymxS0SunI39ISKfzv6f7BCRh0QkOFP74yz32Zh0H0jG17I1bRORNdNcx/Tc78MYM6v/ABs4ACwC/MDrwMoZeN9aYE32cZTM/RNWAv8LuDe7/F7gizO0H/4S+B7wePb5D4A7s4+/AfzRDNTwAPB72cd+ID7T+4PM7NSHgNCE/fCJmdofwDXAGmDHhGWT7gPgVjIzbQuwDtg8zXXcDDjZx1+cUMfK7OcmADRlP0/2eb/XdP9inccPeyXw5ITn9wH3zUIdjwI3AXuA2uyyWmDPDLx3PfAMcD3wePaXqnvCf/gp+2iaaohlP3xy2vIZ3R+cnLa+nMz0d48D757J/QE0nvbhm3QfAP8OfGSy9aajjtNe+wDwYPbxKZ8Z4EngyvN9n0I4HDjvexVMFxFpBFYDm4EaY0xb9qV2oGYGSvgKmYlbvezzCqDfnLzBy0zskyagC/hO9rDkmyJSwgzvD2NMK/Al4CjQBgwArzDz+2Ois+2D2fzdndL9PiZTCCEwq0QkAvwI+AtjzODE10wmVqf1HKqIvA/oNMa8Mp3vcx4cMs3PrxtjVpO5luOU/pkZ2h9lZO5k1QTMB0o48zZ4s2Ym9sG55HK/j8kUQgjM2r0KRMRHJgAeNMY8kl3cISK12ddrgc5pLmM98H4ROQw8TOaQ4KtAXETenA16JvZJC9BijNmcfb6RTCjM9P64EThkjOkyxqSAR8jso5neHxOdbR/M+O/uhPt93JUNpJzrKIQQeBlYku399ZO5oelj0/2mkpkr/VvALmPMlye89Bhwd/bx3WT6CqaNMeY+Y0y9MaaRzM/+rDHmLmATcMcM1tEOHBORZdlFN5CZOn5G9weZw4B1IhLO/h+9WceM7o/TnG0fPAb8bvYswTpgYMJhQ95N2/0+prOT5210gNxKpnf+APC3M/SeV5Np1m0DXsv+u5XM8fgzwD7gaaB8BvfDdZw8O7Ao+x+5H/ghEJiB938nsCW7T34ClM3G/gC+AOwGdgD/j0yv94zsD+AhMn0RKTKto3vOtg/IdOD+7+zv7XZg7TTXsZ/Msf+bv6/fmLD+32br2AO85+28lw4bVqrIFcLhgFJqFmkIKFXkNASUKnIaAkoVOQ0BpYqchoBSRU5DQKki9/8B5mP4engXi2cAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}],"source":["mnist_canvas = get_random_canvas()\n","print(repr(mnist_canvas.boxes[0]))\n","mnist_canvas.plot(mnist_canvas.boxes[0:1])"]},{"cell_type":"code","source":["ts = mnist_canvas.get_torch_tensor()\n","print(ts.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1V5GdPj73Ues","executionInfo":{"status":"ok","timestamp":1671109983873,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}},"outputId":"20d87c6f-bf58-48e9-d524-102e65bc1c6c"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 128, 128])\n"]}]},{"cell_type":"markdown","metadata":{"id":"OgU2bKyJ3pVh"},"source":["For training one can either:\n","- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n","- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."]},{"cell_type":"markdown","metadata":{"id":"_GpJZUkDGJSi"},"source":["### Model building (5 pt.)\n","\n","\n","One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:"]},{"cell_type":"markdown","metadata":{"id":"5qdCBH9PkT2C"},"source":["#### $\\texttt{backbone}$:\n","\n","We provided you with a backbone model architecture paired with Feature Pyramid Network (`BackboneWithFPN`) that accepts a `MnistCanvas` instance and output a dictionary, which has a FPN group name as a keys and their tensors as value.\n","For a FPN with strides set to [32, 64, 128] and number of output channels set to 64, the sizes of the tensors will be [1, 64, 128, 128], [1, 64, 64, 64], [1, 64, 32, 32] consecutively. This module should be trained together with the rest of your solution."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"kXco8riNGHhl","executionInfo":{"status":"ok","timestamp":1671109984369,"user_tz":-60,"elapsed":501,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["from collections import OrderedDict\n","from torch import nn, Tensor\n","from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n","\n","\n","class Backbone(torch.nn.Module):\n","    def __init__(self, strides = [8, 16, 32]):\n","        super().__init__()\n","        \n","        self.first_block = torch.nn.Sequential(\n","            nn.Conv2d(1, strides[0], (3, 3), padding=1),\n","            nn.ReLU(),\n","        )\n","        \n","        self.blocks = torch.nn.ModuleList(\n","            [torch.nn.Sequential(*[\n","                nn.Conv2d(strides[i-1], strides[i], (3, 3), padding=1),\n","                nn.ReLU(),\n","                nn.MaxPool2d(2, 2),\n","              ]) for i in range(1, len(strides))\n","            ]\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        image = x.to(DEVICE).view(1, 1, 128, 128)\n","        x = self.first_block(image)\n","        aux = [x]\n","        for block in self.blocks:\n","            x = block(aux[-1])\n","            aux.append(x)\n","        return aux\n","\n","\n","class BackboneWithFPN(torch.nn.Module):\n","    def __init__(self, strides, out_channels=32) -> None:\n","        super().__init__()\n","        self.strides = strides\n","        self.out_channels = out_channels\n","        self.backbone = Backbone(self.strides)\n","        self.fpn = FeaturePyramidNetwork(self.strides, self.out_channels)\n","\n","    def forward(self, x: torch.Tensor):\n","        output_backbone = self.backbone(x)\n","        \n","        x = OrderedDict()\n","        for i, f in enumerate(output_backbone):\n","            x[f'feat{i}'] = f\n","        output_fpn = self.fpn(x)\n","        return output_fpn"]},{"cell_type":"markdown","metadata":{"id":"JkOLl12nkiI8"},"source":["#### $\\texttt{anchor generator}$:\n","\n","FCOS is anchor-free in a typical sense of this word, but it can also be said that there is one pixel-wise \"anchor\" per localisation on a given feature map.\n","Therefore, anchor generator from `torchvision` is used for convenience.\n","You will obtain $128^2 + 64^2 + 32^2 = 21504$ locations in total for the previously chosen strides.\n","They will be called anchors in the code."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dyd2-zOxf9VM","outputId":"0716d0a4-f29b-4baf-d89d-7380b394c46e","executionInfo":{"status":"ok","timestamp":1671109984370,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1]"]},"metadata":{},"execution_count":49}],"source":["# example code - anchor generator is already included in the code later\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","\n","anchor_sizes = ((32,), (64,), (128,))  # equal to strides of FPN multi-level feature map\n","aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor for each level\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","anchor_generator.num_anchors_per_location()\n","# notice that effectively one anchor is one location"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"iM3oSesif-F5","executionInfo":{"status":"ok","timestamp":1671109984370,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["# Later in the code you will use the anchor generator in the following way:\n","# anchors = anchor_generator(images, features)\n","# [x.size(2) * x.size(3) for x in features] # recover level sizes"]},{"cell_type":"markdown","metadata":{"id":"MTYyCuMdmBk7"},"source":["#### $\\texttt{FCOSClassificationHead}$ (1 pt.):\n","\n","Write a classification head to be used in FCOS.\n","The input is is the output of `BackboneWithFPN` forward call.\n","This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$).\n","Each convolutional layer should input and output `self.in_channels` channels.\n","The additional final block should be `nn.Conv2d` outputting `C` channels.\n","The final output should consist of classification logits of shape `(N, A, C)`, where `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"kpDP9QVjnn05","executionInfo":{"status":"ok","timestamp":1671109984371,"user_tz":-60,"elapsed":9,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["class FCOSClassificationHead(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        num_classes: int,\n","        num_convs: int = 4,\n","    ) -> None:\n","        super().__init__()\n","\n","        # TODO: your code here\n","        ################################################################################################\n","        conv_blocks = []\n","        for _ in range(num_convs):\n","            conv_blocks.extend(\n","                [\n","                    nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","                    nn.GroupNorm(32, in_channels),\n","                    nn.ReLU(),\n","                ]\n","            )\n","        self.conv_blocks = nn.Sequential(*conv_blocks)\n","\n","        self.final_conv = nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=1)\n","\n","        self.num_classes = num_classes # to delete\n","\n","\n","        ################################################################################################\n","        # end of your code\n","\n","    def forward(self, x: List[Tensor]) -> Tensor:\n","        # TODO: your code here\n","        ################################################################################################\n","        # output = torch.tensor([], dtype=float).to(DEVICE)\n","\n","        # for input in x:\n","        #     input = self.conv_blocks(input)\n","        #     input = self.final_conv(input)\n","\n","        #     input = torch.flatten(input, start_dim=2, end_dim=3)\n","        #     output = torch.cat((output, input), 2)\n","\n","\n","\n","        # output = torch.permute(output, (0, 2, 1))\n","\n","        # return output\n","\n","        all_cls_logits = []\n","\n","        for features in x:\n","            cls_logits = self.conv_blocks(features)\n","            cls_logits = self.final_conv(cls_logits)\n","\n","            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n","            N, _, H, W = cls_logits.shape\n","            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n","            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n","            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, 4)\n","\n","            all_cls_logits.append(cls_logits)\n","\n","        return torch.cat(all_cls_logits, dim=1)\n","\n","\n","        \n","        ################################################################################################\n","        # end of your code\n"]},{"cell_type":"markdown","metadata":{"id":"QcHAnQy2mFJU"},"source":["#### $\\texttt{FCOSRegressionHead}$  (1 pt.):\n","\n","Write a regression head to be used in FCOS - both for bounding boxes and center-ness.\n","The input is the output of `BackboneWithFPN` forward call.\n","This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$), which will be shared for regression and center-ness.\n","Each convolutional layer should input and output `self.in_channels` channels.\n","The final block for bounding box regression should be `nn.Conv2d` and have `4` channels and it should be followed by relu functional to get rid of negative values.\n","The final block for center-ness regression should be `nn.Conv2d` and have `1` channel.\n","The output should consist of a tuple of tensors (bounding box regression and center-ness).\n","Bounding box regression logits should be of shape `(N, A, 4)`, whereas for center-ness that would be `(N, A, 1)`.\n","Similarly, `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."]},{"cell_type":"code","execution_count":52,"metadata":{"id":"3H6KNzi1mEhg","executionInfo":{"status":"ok","timestamp":1671109984371,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["class FCOSRegressionHead(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        num_convs: int = 4,\n","    ):\n","        super().__init__()\n","        # TODO: your code here\n","        ################################################################################################\n","        conv_blocks = []\n","        for _ in range(num_convs):\n","            conv_blocks.extend(\n","                [\n","                    nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","                    nn.GroupNorm(32, in_channels),\n","                    nn.ReLU(),\n","                ]\n","            )\n","        self.conv_blocks = nn.Sequential(*conv_blocks)\n","        self.regression_final = nn.Sequential(\n","            nn.Conv2d(in_channels, 4, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","        )\n","        self.centerness_final = nn.Conv2d(in_channels, 1, kernel_size=3, stride=1, padding=1)\n","        \n","        \n","        # end of your code\n","        ################################################################################################\n","        \n","\n","    def forward(self, x: List[Tensor]) -> Tuple[Tensor, Tensor]:\n","        # TODO: your code here\n","        ################################################################################################\n","        # regression_output = torch.tensor([], dtype=float).to(DEVICE)\n","        # centerness_output = torch.tensor([], dtype=float).to(DEVICE)\n","        # for input in x:\n","        #     # inputs are f.e [1, 64, 128, 128], [1, 64, 64, 64] etc\n","        #     input = self.conv_blocks(input)\n","        #     regression = self.regression_final(input)\n","        #     centerness = self.centerness_final(input)\n","\n","        #     regression = torch.flatten(regression, start_dim=2, end_dim=3)\n","        #     centerness = torch.flatten(centerness, start_dim=2, end_dim=3)\n","\n","        #     regression_output = torch.cat((regression_output,regression), 2)\n","        #     centerness_output = torch.cat((centerness_output,centerness), 2)\n","\n","        # regression_output = torch.permute(regression_output, (0, 2, 1))\n","        # centerness_output = torch.permute(centerness_output, (0, 2, 1))\n","\n","        # return (regression_output, centerness_output)\n","\n","        all_bbox_regression = []\n","        all_bbox_ctrness = []\n","\n","        for features in x:\n","            bbox_feature = self.conv_blocks(features)\n","            bbox_regression = self.regression_final(bbox_feature)\n","            bbox_ctrness = self.centerness_final(bbox_feature)\n","\n","            # permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n","            N, _, H, W = bbox_regression.shape\n","            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n","            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n","            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n","            all_bbox_regression.append(bbox_regression)\n","\n","            # permute bbox ctrness output from (N, 1 * A, H, W) to (N, HWA, 1).\n","            bbox_ctrness = bbox_ctrness.view(N, -1, 1, H, W)\n","            bbox_ctrness = bbox_ctrness.permute(0, 3, 4, 1, 2)\n","            bbox_ctrness = bbox_ctrness.reshape(N, -1, 1)\n","            all_bbox_ctrness.append(bbox_ctrness)\n","\n","        return torch.cat(all_bbox_regression, dim=1), torch.cat(all_bbox_ctrness, dim=1)\n","        \n","        ################################################################################################\n","        # end of your code"]},{"cell_type":"markdown","metadata":{"id":"ds0I47ydkvUL"},"source":["#### $\\texttt{FCOSHead}$ (2 pt.):\n","\n","Here, the computation of the foreground indices and losses takes place.\n","\n","##### Loss calculation\n","Compute the losses. \n","They should be calculated on the positive locations/anchors, so use the foreground mask.\n","For regression, use `self.box_coder.encode_single` and `self.box_coder.decode_single` to move between standard (x, y, x, y) and FCOS (l, t, r, b) bounding box format.\n","There are three losses to be written.\n","- classification loss (with `torchvision.ops.sigmoid_focal_loss`). (1 pt.)\n","- Bounding box regression (with `torchvision.ops.generalized_box_iou_loss`). Decode predictions with `self.box_coder.decode_single` before regressing against the ground truth. (1 pt.)\n","- ctrness loss (`torchvision.ops.sigmoid_focal_loss`). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"PX8P0s-jOK-F","executionInfo":{"status":"ok","timestamp":1671109984371,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["class BoxLinearCoder:\n","    \"\"\"\n","    The linear box-to-box transform defined in FCOS. The transformation is parameterized\n","    by the distance from the center of (square) src box to 4 edges of the target box.\n","    \"\"\"\n","\n","    def __init__(self, normalize_by_size: bool = True) -> None:\n","        \"\"\"\n","        Args:\n","            normalize_by_size (bool): normalize deltas by the size of src (anchor) boxes.\n","        \"\"\"\n","        self.normalize_by_size = normalize_by_size\n","\n","    def encode_single(self, reference_boxes: Tensor, proposals: Tensor) -> Tensor:\n","        \"\"\"\n","        Encode a set of proposals with respect to some reference boxes\n","\n","        Args:\n","            reference_boxes (Tensor): reference boxes\n","            proposals (Tensor): boxes to be encoded\n","\n","        Returns:\n","            Tensor: the encoded relative box offsets that can be used to\n","            decode the boxes.\n","        \"\"\"\n","        # get the center of reference_boxes\n","        reference_boxes_ctr_x = 0.5 * (reference_boxes[:, 0] + reference_boxes[:, 2])\n","        reference_boxes_ctr_y = 0.5 * (reference_boxes[:, 1] + reference_boxes[:, 3])\n","\n","        # get box regression transformation deltas\n","        target_l = reference_boxes_ctr_x - proposals[:, 0]\n","        target_t = reference_boxes_ctr_y - proposals[:, 1]\n","        target_r = proposals[:, 2] - reference_boxes_ctr_x\n","        target_b = proposals[:, 3] - reference_boxes_ctr_y\n","\n","        targets = torch.stack((target_l, target_t, target_r, target_b), dim=1)\n","        if self.normalize_by_size:\n","            reference_boxes_w = reference_boxes[:, 2] - reference_boxes[:, 0]\n","            reference_boxes_h = reference_boxes[:, 3] - reference_boxes[:, 1]\n","            reference_boxes_size = torch.stack(\n","                (reference_boxes_w, reference_boxes_h, reference_boxes_w, reference_boxes_h), dim=1\n","            )\n","            targets = targets / reference_boxes_size\n","\n","        return targets\n","\n","    def decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n","        \"\"\"\n","        From a set of original boxes and encoded relative box offsets,\n","        get the decoded boxes.\n","\n","        Args:\n","            rel_codes (Tensor): encoded boxes\n","            boxes (Tensor): reference boxes.\n","\n","        Returns:\n","            Tensor: the predicted boxes with the encoded relative box offsets.\n","        \"\"\"\n","\n","        boxes = boxes.to(rel_codes.dtype)\n","\n","        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n","        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n","        if self.normalize_by_size:\n","            boxes_w = boxes[:, 2] - boxes[:, 0]\n","            boxes_h = boxes[:, 3] - boxes[:, 1]\n","            boxes_size = torch.stack((boxes_w, boxes_h, boxes_w, boxes_h), dim=1)\n","            rel_codes = rel_codes * boxes_size\n","\n","        pred_boxes1 = ctr_x - rel_codes[:, 0]\n","        pred_boxes2 = ctr_y - rel_codes[:, 1]\n","        pred_boxes3 = ctr_x + rel_codes[:, 2]\n","        pred_boxes4 = ctr_y + rel_codes[:, 3]\n","        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=1)\n","        return pred_boxes"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"nDX5c0lw9ZxM","executionInfo":{"status":"ok","timestamp":1671109984372,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["from collections import OrderedDict\n","from functools import partial\n","from typing import Dict, List, Tuple, Optional\n","\n","import torch\n","from torch import nn, Tensor\n","\n","from torchvision.ops import sigmoid_focal_loss, generalized_box_iou_loss\n","from torchvision.ops import boxes as box_ops\n","from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","\n","class FCOSHead(nn.Module):\n","    \"\"\"\n","    A regression and classification head for use in FCOS.\n","\n","    Args:\n","        in_channels (int): number of channels of the input feature\n","        num_classes (int): number of classes to be predicted\n","        num_convs (Optional[int]): number of conv layer of head. Default: 4.\n","    \"\"\"\n","    def __init__(self, in_channels: int, num_classes: int, num_convs: Optional[int] = 4) -> None:\n","        super().__init__()\n","        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n","        self.classification_head = FCOSClassificationHead(in_channels, num_classes, num_convs)\n","        self.regression_head = FCOSRegressionHead(in_channels, num_convs)\n","\n","    def compute_loss(\n","        self,\n","        targets: List[Dict[str, Tensor]],\n","        head_outputs: Dict[str, Tensor],\n","        anchors: List[Tensor],             # anchors/locations\n","        matched_idxs: List[Tensor],        # tells to which bounding box anchors are matched, -1 mean no matches\n","    ) -> Dict[str, Tensor]:\n","\n","        cls_logits = head_outputs[\"cls_logits\"]  # [N, A, C]\n","        bbox_regression = head_outputs[\"bbox_regression\"]  # [N, A, 4]\n","        bbox_ctrness = head_outputs[\"bbox_ctrness\"]  # [N, A, 1]\n","\n","        all_gt_classes_targets = []\n","        all_gt_boxes_targets = []\n","        \n","        for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):\n","            gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n","            # tensor of size [A, 4], meaning that gt_boxes_targets[a] is a box matched to ath anchor\n","            gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n","            gt_classes_targets[matched_idxs_per_image < 0] = -1  # background\n","            all_gt_classes_targets.append(gt_classes_targets)\n","            # list of length (number of images) of tensors of size [A,4] as above\n","            all_gt_boxes_targets.append(gt_boxes_targets)\n","\n","        # [N, A], all_gt_classes_targets[n, i] = class label of ith anchor in nth image\n","        all_gt_classes_targets = torch.stack(all_gt_classes_targets)\n","        \n","        # [N, A] \\in {0, 1}, foreground_max[n, i], ith anchor of nth image is positive\n","        foregroud_mask = all_gt_classes_targets >= 0        \n","        num_foreground = foregroud_mask.sum().item()\n","\n","        \n","        loss_cls = self.compute_loss_cls(cls_logits, all_gt_classes_targets, foregroud_mask)\n","        loss_bbox_reg = self.compute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n","        loss_bbox_ctrness = self.compute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n","\n","        return {\n","            \"classification\": loss_cls / max(1, num_foreground),\n","            \"bbox_regression\": loss_bbox_reg / max(1, num_foreground),\n","            \"bbox_ctrness\": loss_bbox_ctrness / max(1, num_foreground),\n","        }\n","\n","    def compute_loss_ctrness(self, anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask):\n","         # TODO: your code here \n","        # gets bboxes of size [N, 4] (N bbox coordinates encoded in ltrb) and computes a tensor of size [N] with their centerness\n","        def compute_cntrness(bboxes: torch.Tensor) -> torch.Tensor:\n","            lr = bboxes[:, [0, 2]]\n","            tb = bboxes[:, [1, 3]]\n","            nominator = torch.min(lr, dim=1)[0] * torch.min(tb, dim=1)[0]\n","            denominator =  torch.max(tb, dim=1)[0] * torch.max(lr, dim=1)[0]\n","            denominator = torch.clip(denominator, min=1e-7)\n","\n","            return torch.unsqueeze(torch.sqrt(nominator / denominator), -1)\n","\n","        # ctrness loss\n","        ################################################################################################ \n","        loss = 0.0\n","        for one_img_targets, one_box_ctrness, single_foreground_mask, single_anchors in zip(all_gt_boxes_targets, bbox_ctrness, foregroud_mask, anchors):\n","            # tensor of size [A, 4] with boxes encoded as ltrb\n","            encoded_targets = self.box_coder.encode_single(single_anchors, one_img_targets)\n","            ctrness_targets = compute_cntrness(encoded_targets[single_foreground_mask])\n","\n","            loss += sigmoid_focal_loss(one_box_ctrness[single_foreground_mask], ctrness_targets, reduction='sum')\n","\n","        return loss\n","\n","        ################################################################################################\n","\n","    def compute_loss_bbox_reg(self, anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask):\n","         # TODO: your code here \n","        # regression loss: GIoU loss\n","        ################################################################################################\n","        loss = 0.0\n","        for one_img_targets, one_box_regression, single_foreground_mask, single_anchors in zip(all_gt_boxes_targets, bbox_regression, foregroud_mask, anchors):\n","            decoded_output = self.box_coder.decode_single(one_box_regression, single_anchors)\n","            loss += generalized_box_iou_loss(decoded_output[single_foreground_mask], one_img_targets[single_foreground_mask], reduction='sum')\n","\n","        return loss\n","        ################################################################################################\n","\n","    def compute_loss_cls(self, cls_logits, all_gt_classes_targets, foregroud_mask):\n","        # TODO: your code here \n","        # classification loss\n","        ################################################################################################\n","        # want to change all_gt_classes_targets[n, i] = class label of ith anchor in nth image to [N, A, C]\n","        # https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-one-hot-encoded-array-in-numpy\n","        # instead of having a class label for each anchor, we want to have a one hot encoded 1D tensor for each anchor\n","        # we could do it like this one_hot(x + 1)[:, :, 1:] (1: because we need to get rid of values that meant -1, cause it's redundant)\n","        # but this feels more elegant\n","        # first we copy the shape from cls_logits. Then we apply foreground mask, to select appropriate 1D to be one-hot encoded tensors,\n","        # and then choose appropriate positions (again we apply foreground mask, since we don't want to have -1 index value)\n","        class_targets_encoded = torch.zeros_like(cls_logits)\n","        class_targets_encoded[foregroud_mask, all_gt_classes_targets[foregroud_mask]] = 1. \n","\n","        return sigmoid_focal_loss(cls_logits, class_targets_encoded, reduction='sum')\n","        \n","\n","        \n","        ################################################################################################\n","\n","    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n","        cls_logits = self.classification_head(x)\n","        bbox_regression, bbox_ctrness = self.regression_head(x)\n","        return {\n","            \"cls_logits\": cls_logits,\n","            \"bbox_regression\": bbox_regression,\n","            \"bbox_ctrness\": bbox_ctrness,\n","        }"]},{"cell_type":"markdown","metadata":{"id":"gFYNzC9KOK-G"},"source":["#### Post-processing (1 pt.)\n","Fill the gaps in the postprocessing routine.\n","The paper states: \n","\n","> (...) the final score (used for ranking the detected bounding boxes) \n","> is computed by multiplying the predicted center-ness with the corresponding classification score. \n","> Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. \n","> As a result, with high probability, these low-quality bounding boxes might be filtered out by \n","> the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n","\n","1. Remove boxes with score smaller than `self.score_thresh`. The score is given by `sqrt`($\\sigma$(`classification_score`) * $\\sigma$(`cente-ness_score`)), where $\\sigma$ stands for the sigmoid function. (1pt.)\n","2. Keep only top `self.topk_candidates` scoring predictions (1pt.)\n","\n","The `compute_loss` function here calculates the indexes of matched classes for each anchor/location for your convenience in later calculations."]},{"cell_type":"code","execution_count":55,"metadata":{"id":"EB17o3zs1JiR","executionInfo":{"status":"ok","timestamp":1671109984372,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["class FCOS(nn.Module):\n","\n","    def __init__(\n","        self,\n","        backbone: nn.Module,\n","        num_classes: int,\n","        # transform parameters\n","        image_mean: Optional[List[float]] = None,\n","        image_std: Optional[List[float]] = None,\n","        # Anchor parameters\n","        anchor_generator: AnchorGenerator = None,\n","        center_sampling_radius: float = 1.5,\n","        score_thresh: float = 0.2,\n","        nms_thresh: float = 0.6,\n","        detections_per_img: int = 100,\n","        topk_candidates: int = 1000,\n","        num_convs_in_heads:int = 4,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","\n","        self.backbone = backbone\n","        self.anchor_generator = anchor_generator\n","        self.head = FCOSHead(backbone.out_channels, num_classes, num_convs=num_convs_in_heads)\n","        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n","        self.transform = GeneralizedRCNNTransform(128, 128, image_mean, image_std, **kwargs)\n","\n","        self.center_sampling_radius = center_sampling_radius\n","        self.score_thresh = score_thresh\n","        self.nms_thresh = nms_thresh\n","        self.detections_per_img = detections_per_img\n","        self.topk_candidates = topk_candidates\n","\n","\n","    def compute_loss(\n","        self,\n","        targets: List[Dict[str, Tensor]],\n","        head_outputs: Dict[str, Tensor],\n","        anchors: List[Tensor],\n","        num_anchors_per_level: List[int],\n","    ) -> Dict[str, Tensor]:\n","        matched_idxs = []\n","        for anchors_per_image, targets_per_image in zip(anchors, targets): # batch\n","            if targets_per_image[\"boxes\"].numel() == 0:\n","                matched_idxs.append(\n","                    torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device)\n","                )\n","                continue\n","\n","            gt_boxes = targets_per_image[\"boxes\"]\n","            gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2                 # Calculate centres of bounding boxes\n","            anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N  \n","            anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]            # Match anchors\n","            # center sampling: anchor point must be close enough to gt center.\n","            pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n","                dim=2\n","            ).values < self.center_sampling_radius * anchor_sizes[:, None]\n","            # compute pairwise distance between N points and M boxes\n","            x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n","            x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n","            pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n","\n","            # anchor point must be inside gt\n","            pairwise_match &= pairwise_dist.min(dim=2).values > 0\n","\n","            # each anchor is only responsible for certain scale range.\n","            lower_bound = anchor_sizes * 4\n","            lower_bound[: num_anchors_per_level[0]] = 0\n","            upper_bound = anchor_sizes * 8\n","            upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n","            pairwise_dist = pairwise_dist.max(dim=2).values\n","            pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])\n","\n","            # match the GT box with minimum area, if there are multiple GT matches\n","            gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n","            pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n","            min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n","            matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n","\n","            matched_idxs.append(matched_idx)\n","        # end of your code\n","        # matched index - anchor-to-target match\n","        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n","\n","    def postprocess_detections(\n","        self, head_outputs: Dict[str, List[Tensor]], anchors: List[List[Tensor]], image_shapes: List[Tuple[int, int]]\n","    ) -> List[Dict[str, Tensor]]:\n","       #[N, A, C]\n","        class_logits = head_outputs[\"cls_logits\"]\n","        # [N, A, 4]\n","        box_regression = head_outputs[\"bbox_regression\"]\n","        # [N, A, 1]\n","        box_ctrness = head_outputs[\"bbox_ctrness\"]\n","\n","        #pass # TODO: your code here ?????\n","        num_images = len(image_shapes)\n","\n","        detections: List[Dict[str, Tensor]] = []\n","\n","        for index in range(num_images):\n","            box_regression_per_image = [br[index] for br in box_regression]\n","            logits_per_image = [cl[index] for cl in class_logits]\n","            box_ctrness_per_image = [bc[index] for bc in box_ctrness]\n","            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n","\n","            image_boxes = []\n","            image_scores = []\n","            image_labels = []\n","\n","            for box_regression_per_level, logits_per_level, box_ctrness_per_level, anchors_per_level in zip(\n","                box_regression_per_image, logits_per_image, box_ctrness_per_image, anchors_per_image\n","            ):\n","                num_classes = logits_per_level.shape[-1]\n","                \n","                 # TODO: your code here\n","                # Remove low scoring boxes and keep only topk scoring predictions\n","                ################################################################################################\n","\n","                scores_per_level = torch.sqrt(torch.sigmoid(box_ctrness_per_level) * torch.sigmoid(logits_per_level)).flatten()\n","                mask = scores_per_level > self.score_thresh\n","\n","                scores_per_level = scores_per_level[mask]\n","                # original_idxs[i] is the original idx of scores_per_level[i]\n","                topk_idxs = torch.where(mask)[0]\n","                scores_per_level, idxs = torch.topk(scores_per_level, min(self.topk_candidates, topk_idxs.size()[0]))\n","                topk_idxs = topk_idxs[idxs]\n","                ################################################################################################\n","                # end of your code\n","                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n","                labels_per_level = topk_idxs % num_classes\n","\n","                boxes_per_level = self.box_coder.decode_single(\n","                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n","                )\n","                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n","\n","                image_boxes.append(boxes_per_level)\n","                image_scores.append(scores_per_level)\n","                image_labels.append(labels_per_level)\n","\n","            image_boxes = torch.cat(image_boxes, dim=0)\n","            image_scores = torch.cat(image_scores, dim=0)\n","            image_labels = torch.cat(image_labels, dim=0)\n","\n","            # non-maximum suppression\n","            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n","            keep = keep[: self.detections_per_img]\n","\n","            detections.append(\n","                {\n","                    \"boxes\": image_boxes[keep],\n","                    \"scores\": image_scores[keep],\n","                    \"labels\": image_labels[keep],\n","                }\n","            )\n","        return detections\n","\n","\n","    def forward(\n","        self,\n","        images: List[Tensor],\n","        targets: Optional[List[Dict[str, Tensor]]] = None,\n","    ) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]:\n","        \"\"\"\n","        Args:\n","            images (list[Tensor]): images to be processed\n","            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n","\n","        Returns:\n","            result (list[BoxList] or dict[Tensor]): the output from the model.\n","                During training, it returns a dict[Tensor] which contains the losses.\n","                During testing, it returns list[BoxList] contains additional fields\n","                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","        \"\"\"\n","        \n","        # transform the input (normalise with std and )\n","        images, targets = self.transform(images, targets)\n","\n","        # get the features from the backbone\n","        features = self.backbone(images.tensors)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        features = list(features.values())\n","\n","        # compute the fcos heads outputs using the features\n","        head_outputs = self.head(features)\n","\n","        # create the set of anchors\n","        anchors = self.anchor_generator(images, features)\n","        # recover level sizes\n","        num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n","        \n","        losses = {}\n","        detections: List[Dict[str, Tensor]] = []\n","        if self.training:\n","            losses = self.compute_loss(targets, head_outputs, anchors, num_anchors_per_level)\n","            return losses\n","        else:\n","            # split outputs per level\n","            split_head_outputs: Dict[str, List[Tensor]] = {}\n","            for k in head_outputs:\n","                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n","            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n","\n","            # compute the detections\n","            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n","            return detections"]},{"cell_type":"markdown","metadata":{"id":"xA1Nvz6jagyP"},"source":["### Metrics and evaluation (2 pt.)\n","\n","#### Digit Accuracy (1 pt.)\n","\n","This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]`, and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n","\n","- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class and `iou` overlap greater than `0.5`,\n","- the number of `canvas` boxes match `len(predicted_boxes)`.\n","\n","The method shoud output `1` if there is a matching and `0` otherwise.\n","\n","#### Evaluation function (1 pt.)\n","\n","Write an evaluation function for your model.\n","If needed, perform the final NMS with `torchvision.ops.nms` (threshold at `iou`) and remove redundant boxes with scores smaller than `min_score`.\n","Then, calculate the average `DigitAccuracy` for each.\n","You may experiment with parameters for this method, but the default ones are fine (this is not subject of our grading).\n","If you are fine what you achiveded with `postprocess_detections`, you may focus solely on evaluation (although playing with this second stage NMS and score thresholding might be useful for diagnostics).\n","\n","The output of the method is the average digit accuracy on the test set. \n","Use it to track your model performance over epochs.\n","\n","In principle, you can use different `iou` and `min_score`, although it’d be slightly preferred to use the defaults here. It’ll ease the comparison, we were able to get around 60% in 15 epochs with these values. With that said, if you tune these values for your model to improve the score there will be no point lost.\n","…unless you will do some sort of a hack to exploit the metric in a malicious way, but that will perhaps mean that there’s something wrong in other parts of the code."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"bfRZnbAP-XGG","executionInfo":{"status":"ok","timestamp":1671109984372,"user_tz":-60,"elapsed":7,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"outputs":[],"source":["class DigitAccuracy:\n","    def compute_metric  (\n","        self,\n","        predicted_boxes: List[MnistBox],\n","        canvas: MnistCanvas,\n","        iou: float = 0.5,\n","    ):\n","        # TODO: your code here\n","        # we have only 3-6 gt boxes, we can do it in double loop\n","        ################################################################################################    \n","        if len(predicted_boxes) != len(canvas.boxes):\n","            return 0\n","\n","        used_pred_boxes = [False] * len(canvas.boxes) # used_gt_boxes[i] means ith gt_box has a pair\n","        for gt_box in canvas.boxes:\n","            for idx, pred_box in enumerate(predicted_boxes):\n","                if gt_box.class_nb == pred_box.class_nb and gt_box.iou_with(pred_box) > iou:\n","                    if used_pred_boxes[idx]:\n","                        return 0\n","                    else:\n","                        used_pred_boxes[idx] = True\n","        return 1\n","\n","\n","\n","        \n","        ################################################################################################\n","        # end of your code\n","\n","def evaluate(model: nn.Module, TEST_CANVAS, min_score: float = .2, iou: float = .05) -> float:\n","    # TODO: your code here\n","    ################################################################################################\n","    da = DigitAccuracy()\n","    model.eval()\n","    accuracy_sum = 0\n","    foo = 0\n","    with torch.no_grad():\n","        for canvas in TEST_CANVAS:\n","            foo += 1\n","            image = torch.squeeze(canvas.get_torch_tensor(), 0).to(DEVICE)\n","            predictions = model([image])[0]\n","            predicted_boxes = [MnistBox(box[1].item(), box[0].item(), box[1].item(), box[0].item(), label.item()) for box, label in zip(predictions['boxes'], predictions['labels'])]\n","\n","            accuracy_sum += da.compute_metric(predicted_boxes, canvas)\n","\n","            if foo % 50 == 0:\n","                canvas.plot(predicted_boxes)\n","\n","    print(f'\\n Test Set Accuracy: {accuracy_sum}/{len(TEST_CANVAS)} ({100.0*(accuracy_sum/len(TEST_CANVAS)):.0f}%)\\n')\n","\n","\n","    ################################################################################################\n","    # end of your code"]},{"cell_type":"markdown","metadata":{"id":"15n5w-hhvRbS"},"source":["### Train your model (3pt)\n","\n","One should use all classes defined above to train the model.\n","\n","- Train the model. A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data (2 pt.).\n","- Plot example results of matched and mismatched predictions (0.5 pt.).\n","- Plot particular losses and evaluation score per (0.5 pt.).\n","\n","What does the target variable look like?\n","So, as in the typehint, the target is `List[Dict[str, Tensor]]`. Assuming a 1-element batch, it should then be a 1-element list containing a dictionary with two keys: `boxes` and their corresponding `labels`. The values are tensors. Assuming that we have 5 boxes in GT, the shapes are `(5, 4)` for boxes and (5) for labels. Naturally, you have to fill these tensors using values from `MnistCanvas`.\n","\n","**Hint:** Training can take a while to achieve the expected accuracy. It is normal that for many epochs at the beginning accuracy is constantly $0$. Do not worry as long as the loss is on average decreasing across epochs. You may want to reduce number of digit classes (for example only to generate `0`s on canvas) to test the convergence (the hyperparameters might change, though!). A model with around 500k parameters should be able to hit 10% of the metric in 20 minutes (tested on a 2021 MacBook on CPU). On Google Colab with GPU it will be matter of 2 minutes, but notice that the free GPU is limited.\n","\n","**Hint:** Use the 1-element batches. Some portions of the code are not ready for higher values.\n","\n","**Even more important hint:** Pay attention to the ordering of the X/Y values of `get_torch_tensor` and align it with your target!\n","\n","Good luck and have fun!"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"eu9xbkaY1Jih","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671111280426,"user_tz":-60,"elapsed":3333,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}},"outputId":"6053c357-8cc0-420c-b391-594f1481e32a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["FCOS(\n","  (backbone): BackboneWithFPN(\n","    (backbone): Backbone(\n","      (first_block): Sequential(\n","        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU()\n","      )\n","      (blocks): ModuleList(\n","        (0): Sequential(\n","          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU()\n","          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        )\n","        (1): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU()\n","          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","    )\n","  )\n","  (anchor_generator): AnchorGenerator()\n","  (head): FCOSHead(\n","    (classification_head): FCOSClassificationHead(\n","      (conv_blocks): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (2): ReLU()\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (5): ReLU()\n","        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (7): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (8): ReLU()\n","        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (10): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (11): ReLU()\n","      )\n","      (final_conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","    (regression_head): FCOSRegressionHead(\n","      (conv_blocks): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (2): ReLU()\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (5): ReLU()\n","        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (7): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (8): ReLU()\n","        (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (10): GroupNorm(32, 64, eps=1e-05, affine=True)\n","        (11): ReLU()\n","      )\n","      (regression_final): Sequential(\n","        (0): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU()\n","      )\n","      (centerness_final): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.0233], std=[0.14])\n","      Resize(min_size=(128,), max_size=128, mode='bilinear')\n","  )\n",")"]},"metadata":{},"execution_count":60}],"source":["import torch.optim as optim\n","import torch\n","import torchvision\n","import pandas as pd\n","\n","TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n","np.random.seed(TEST_SEED)\n","\n","N_TRAINING_EXAMPLES = 500\n","LR = 0.0004 # for SGD with momentum=0.9\n","EPOCHS = 100\n","STRIDES = [32, 64, 128]\n","CONVS_IN_HEADS = 4\n","OUT_CHANNELS = 64\n","LABELS = list(range(1))  # lower it for quick convergence testing\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","\n","TRAIN_CANVAS = [\n","    get_random_canvas(\n","        digits=TEST_DIGITS,\n","        classes=TEST_CLASSES,\n","        labels=LABELS,\n","    )\n","    for _ in range(N_TRAINING_EXAMPLES)\n","]\n","\n","TEST_CANVAS_SIZE = 256\n","\n","\n","TEST_CANVAS = [\n","    get_random_canvas(\n","        digits=TEST_DIGITS,\n","        classes=TEST_CLASSES,\n","        labels=LABELS,\n","    )\n","    for _ in range(TEST_CANVAS_SIZE)\n","]\n","\n","\n","anchor_sizes = tuple([(x,) for x in STRIDES])  # equal to strides of multi-level feature map\n","aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one \"anchor\" per location\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","fcos = FCOS(\n","    backbone = BackboneWithFPN(strides=STRIDES, out_channels=OUT_CHANNELS), \n","    num_classes = len(LABELS), \n","    image_mean = [0.0233],\n","    image_std = [0.14],\n","    num_convs_in_heads = CONVS_IN_HEADS, \n","    anchor_generator = anchor_generator,\n","    detections_per_img = 100\n","    )\n","fcos.to(DEVICE)\n"]},{"cell_type":"code","source":["image = TRAIN_CANVAS[0].get_boxes_and_classes()\n","print(image['boxes'].dtype)\n","torch.cat((torch.tensor([]), torch.tensor([[[1,2,3]]])), 2)\n","mask = torch.randint(0, 2, (10,))\n","print(mask)\n","print(torch.where(mask))\n","print(mask.nonzero().flatten())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpGGwHQhqvrA","executionInfo":{"status":"ok","timestamp":1671111280427,"user_tz":-60,"elapsed":8,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}},"outputId":"8990c12c-f487-4321-9592-33a2308db9f2"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int64\n","tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1])\n","(tensor([0, 1, 4, 5, 6, 9]),)\n","tensor([0, 1, 4, 5, 6, 9])\n"]}]},{"cell_type":"code","source":["\n","# TODO: write your code here\n","################################################################################################\n","def train(model, optimizer, epoch, log_interval, train_canvas):\n","    model.train()\n","    for idx, canvas in enumerate(train_canvas):\n","        image = torch.squeeze(canvas.get_torch_tensor(), 0).to(DEVICE)\n","        target = canvas.get_boxes_and_classes()\n","        optimizer.zero_grad()\n","\n","        losses = model([image], [target])\n","        loss = losses['classification'] + losses['bbox_regression'] + losses['bbox_ctrness']\n","        loss.backward()\n","\n","        \n","        optimizer.step()\n","\n","        if idx % log_interval == 0:\n","            print(\n","                f\"Train Epoch: {epoch} [{idx}/{len(train_canvas)}]\\tLosses:\\n\" +\n","                f\"\\t Classification: {losses['classification']:.6f}\\n\"+\n","                f\"\\t bbox regression: {losses['bbox_regression']:.6f}\\n\" +\n","                f\"\\t bbox_ctrness: {losses['bbox_ctrness']:.6f}\"\n","            )\n","\n","optimizer = optim.SGD(fcos.parameters(), lr=LR, momentum=0.9)\n","for epoch in range(1, EPOCHS+1):\n","    train(fcos, optimizer, epoch, 100, TRAIN_CANVAS)\n","    evaluate(fcos, TEST_CANVAS)\n","\n","################################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"isKpX0XBqPlq","outputId":"1cdabcc7-bf58-4473-eb11-d488eeb4c006","executionInfo":{"status":"error","timestamp":1671111324176,"user_tz":-60,"elapsed":43754,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/500]\tLosses:\n","\t Classification: 6.497935\n","\t bbox regression: 1.242621\n","\t bbox_ctrness: 0.096305\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-79ee8d2dd3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_CANVAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_CANVAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-79ee8d2dd3a6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch, log_interval, train_canvas)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classification'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox_regression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox_ctrness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-145321c435a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# compute the fcos heads outputs using the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# create the set of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-1b336cc8b897>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mbbox_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_ctrness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         return {\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-dfdf87bd161d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!nvidia-smi -L  "],"metadata":{"id":"JBfpkeegKDT2","executionInfo":{"status":"aborted","timestamp":1671111324177,"user_tz":-60,"elapsed":6,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JkOLl12nkiI8","QcHAnQy2mFJU"],"provenance":[{"file_id":"https://github.com/mim-uw/dnn-2022-23/blob/master/released/hw2-fcos-student.ipynb","timestamp":1670941147635}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"}}},"nbformat":4,"nbformat_minor":0}