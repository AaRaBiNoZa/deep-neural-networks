{"cells":[{"cell_type":"markdown","metadata":{"id":"gcTwzhX8fBqs"},"source":["Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n","\n","In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n","The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n","\n","Tasks:\n","\n","    1. Implement residual connections in the missing places in the code.\n","\n","    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n","\n","    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYAsziKffBFV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5Krupfv8Oma"},"outputs":[],"source":["class ResidualConnection(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv_block_1 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                kernel_size=3,\n","                padding=1,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","        self.conv_block_2 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=out_channels,\n","                out_channels=out_channels,\n","                kernel_size=3,\n","                padding=1,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","\n","    def forward(self, x):\n","        # TODO: implement forward pass.\n","        r = self.conv_block_1(x)\n","        r = self.conv_block_2(r)\n","        r = r + x\n","\n","        return r"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jpMoIWx8Omb"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.rc = nn.Sequential(\n","            ResidualConnection(1, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","        )\n","        self.fc = nn.Linear(\n","            28 * 28 * 16, 10\n","        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n","\n","    def forward(self, x):\n","        x = self.rc(x)\n","        x = nn.Flatten(start_dim=1)(x)\n","        x = self.fc(x)\n","        output = nn.LogSoftmax(dim=1)(x)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s52hCDGjNHP3"},"outputs":[],"source":["class Net2(nn.Module):\n","    def __init__(self):\n","        super(Net2, self).__init__()\n","        self.rc = nn.Sequential(\n","            ResidualConnection(1, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","            ResidualConnection(16, 16),\n","        )\n","        self.fc = nn.Linear(\n","            28 * 28 * 16, 10\n","        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n","\n","    def forward(self, x):\n","        x = self.rc(x)\n","        x = nn.Flatten(start_dim=1)(x)\n","        x = self.fc(x)\n","        output = nn.LogSoftmax(dim=1)(x)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMtap4QCfBH8"},"outputs":[],"source":["def train(model, device, train_loader, optimizer, epoch, log_interval):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    loss.item(),\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print(\n","        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n","            test_loss,\n","            correct,\n","            len(test_loader.dataset),\n","            100.0 * correct / len(test_loader.dataset),\n","        )\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5GlMs1-fBKP"},"outputs":[],"source":["batch_size = 256\n","test_batch_size = 1000\n","epochs = 3\n","lr = 1e-2\n","use_cuda = False\n","seed = 1\n","log_interval = 10\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgfUP23AfBMd"},"outputs":[],"source":["use_cuda = not use_cuda and torch.cuda.is_available()\n","\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","train_kwargs = {\"batch_size\": batch_size}\n","test_kwargs = {\"batch_size\": test_batch_size}\n","if use_cuda:\n","    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n","    train_kwargs.update(cuda_kwargs)\n","    test_kwargs.update(cuda_kwargs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0KPoUtsfBOs"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",")\n","dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n","dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n","train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n","test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499747,"status":"ok","timestamp":1669638343286,"user":{"displayName":"Adam Al-Hosam","userId":"09700474471122457475"},"user_tz":-60},"id":"ezvIQbgsfBRT","outputId":"1e77334c-b9d5-4c93-e622-4fecf11457fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.769945\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 8.170991\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 4.516145\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 3.071406\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.671197\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.523297\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.550843\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.734527\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.856870\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.582643\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.581930\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.737352\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.191192\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.420518\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.666768\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.385038\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.345726\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.153856\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.287999\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.080335\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.126156\n","Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.381323\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.401470\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.014260\n","\n","Test set: Average loss: 0.2022, Accuracy: 9634/10000 (96%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.259599\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.392605\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.207692\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.327728\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.132192\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.158195\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.117119\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.219251\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.235645\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.129831\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.239884\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.205466\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.068102\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.225714\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.350490\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.218361\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.098512\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.082808\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.091323\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.066069\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.147355\n","Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.228031\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.119077\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.040361\n","\n","Test set: Average loss: 0.1862, Accuracy: 9620/10000 (96%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.224423\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.314904\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.140010\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.073795\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.142346\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.081765\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.121989\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.203479\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.192467\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.052694\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.164756\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.176831\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.056939\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.087102\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.125969\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.133193\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.118784\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.055112\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.095872\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.042891\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.109385\n","Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.097625\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.056457\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.000224\n","\n","Test set: Average loss: 0.1287, Accuracy: 9727/10000 (97%)\n","\n"]}],"source":["model = Net().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch, log_interval)\n","    test(model, device, test_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DQMSSwuifBTo","outputId":"c7780764-18fd-4a42-f476-fac149b5dae6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.490020\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 31.342625\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 15.259869\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 10.297441\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 7.466501\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.653012\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.685375\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.749766\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.995073\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.585227\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.746786\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.281907\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.459500\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.724285\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.018064\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.708227\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.753661\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.826604\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.359523\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.350488\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.517467\n","Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.713778\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.371916\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.028924\n","\n","Test set: Average loss: 0.6201, Accuracy: 9601/10000 (96%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.491809\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.151680\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.855407\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.468508\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.604476\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.465980\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.257502\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.373826\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.959431\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.254682\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.395766\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.308824\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.376581\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.752280\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.818658\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.462941\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.542295\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.456283\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.838692\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.292525\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.272925\n","Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.356654\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.340311\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.000007\n","\n","Test set: Average loss: 0.3328, Accuracy: 9706/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.395503\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.535480\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.669989\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.219310\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.234474\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.535507\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.194058\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.245811\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.470758\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.345214\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.302994\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.250968\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.165464\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.532332\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.226309\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.431521\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.663724\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.260524\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.771498\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.233654\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.222218\n","Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.370650\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.172071\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.000001\n","\n","Test set: Average loss: 0.8119, Accuracy: 9481/10000 (95%)\n","\n"]}],"source":["model = Net2().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch, log_interval)\n","    test(model, device, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JX_2rCycfBWU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mim-uw/dnn-2022-23/blob/master/released/DNN-Lab-7-ResidualBlock-in-Pytorch-student-version.ipynb","timestamp":1669633998844}]},"kernelspec":{"display_name":"Python 3.9.13 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"}}},"nbformat":4,"nbformat_minor":0}